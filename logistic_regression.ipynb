{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece961ce",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e7af2",
   "metadata": {},
   "source": [
    "## 1. Оценивание вероятностей\n",
    "\n",
    "Метод обучения, которые получается при использовании логистической функции потерь, называется логистической регрессией. Основным его свойством является тот факт, что он корректно оценивает вероятность принадлежности объекта к каждому из классов.\n",
    "\n",
    "Пусть в каждой точке пространства объектов $x ∈ X$ задана вероятность $p(y = +1 | x)$ того, что объект $x$ будет принадлежать классу $+1$. Это означает, что мы допускаем наличие в выборке нескольких объектов с одинаковым признаковым описанием, но с разными значениями целевой переменной; причём если устремить количество объекта $x$ в выборке к бесконечности, то доля положительных объектов среди них будет стремиться к $p(y = +1 | x)$.\n",
    "\n",
    "**Пример:** Задача предсказания кликов по рекламным баннерам. При посещении одного и того же сайта один и тот же пользователь может как кликнуть, так и не кликнуть по одному и тому же баннеру, из-за чего в выборке могут появиться одинаковые объекты с разными ответами. При этом важно, чтобы классификатор предсказывал именно вероятности классов — если домножить вероятность первого класса на сумму, которую заплатит заказчик в случае клика, то мы получим матожидание прибыли при показе этого баннера. На основе таких матожиданий можно построить алгоритм, выбирающий баннеры для показа пользователю.\n",
    "\n",
    "Рассмотрим точку $x$ пространства объектов. Как мы договорились, в ней имеется распределение на ответах $p(y = +1 | x)$. Допустим, алгоритм $b(x)$ возвращает числа из отрезка $[0, 1]$.\n",
    "\n",
    "Наша задача — выбрать для него такую процедуру обучения, что в точке $x$ ему будет оптимально выдавать число $p(y = +1 | x)$. Если в выборке объект $x$ встречается n раз с ответами $\\{y_1, . . . , y_n\\}$, то получаем следующее требование:\n",
    "\n",
    "$$arg \\min_{b \\in R} \\ \\frac{1}{n} \\sum_{i = 1}^{n} L(y_i, b) \\approx p(y=+1|x).$$\n",
    "\n",
    "При стремлении $n$ к бесконечности получим, что функционал стремится к матожиданию ошибки:\n",
    "\n",
    "$$arg \\min_{b \\in R} \\ E \\big[ L(y, b) | x \\big] = p(y=+1|x).$$\n",
    "\n",
    "\n",
    "Этим свойством обладает, например, квадратичная функция потерь $L(y, z) = (y − z)^2$, если в ней для положительных объектов использовать истинную метку y = 1, а для отрицательных брать y = 0.\n",
    "\n",
    "Примером функции потерь, которая не позволяет оценивать вероятности, является модуль отклонения $L(y, x) = |y − z|.$ Можно показать, что с точки зрения данной функции оптимальным ответом всегда будет либо ноль, либо единица.\n",
    "\n",
    "Это требование можно воспринимать более просто. Пусть один и тот же объект встречается в выборке 1000 раз, из которых 100 раз он относится к классу +1, и 900 раз - к классу −1. Поскольку это один и тот же объект, классификатор должен выдавать один ответ для каждого из тысячи случаев. Можно оценить матожидание функции потерь в данной точке по 1000 примеров при прогнозе b:\n",
    "\n",
    "$$E \\big[ L(y, b) | x \\big] \\approx \\frac{100}{1000} L(1, b) + \\frac{900}{1000} L(-1, b).$$\n",
    "\n",
    "Наше требование, по сути, означает, что оптимальный ответ с точки зрения этой оценки должен быть равен $1 / 10$:\n",
    "\n",
    "$$arg \\min_{b \\in R} \\big( \\frac{100}{1000} L(1, b) + \\frac{900}{1000} L(-1, b) \\big) = \\frac{1}{10}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33579281",
   "metadata": {},
   "source": [
    "## 2. Правдоподобие и логистические потери\n",
    "\n",
    "Хотя квадратичная функция потерь и приводит к корректному оцениванию вероятностей, она не очень хорошо подходит для решения задачи классификации. Причиной этому в том числе являются и слишком низкие штрафы за ошибку — так, если объект положительный, а модель выдаёт для него вероятность первого класса b(x) = 0, то штраф за это равен всего лишь единице: $(1 − 0)^2 = 1.$\n",
    "\n",
    "Попробуем сконструировать функцию потерь из других соображений.\n",
    "\n",
    "Если алгоритм $b(x) \\in [0, 1]$ действительно выдает вероятности, то они должны согласовываться с выборкой.\n",
    "\n",
    "С точки зрения алгоритма вероятность того, что в выборке встретится объект $x_i$ с классом $y_i$, равна\n",
    "$b(x_i)^{[y_i = +1]} (1 - b(x_i))^{[y_i = -1]}.$\n",
    "\n",
    "Исходя из этого, можно записать правдоподобие выборки (т.е. вероятность получить такую выборку с точки зрения алгоритма):\n",
    "$$Q(b, X) = \\prod_{n = 1}^{l} b(x_i)^{[y_i = +1]} (1 - b(x_i))^{[y_i = -1]}$$\n",
    "\n",
    "Данное правдоподобие можно использовать как функционал для обучения алгоритма — с той лишь оговоркой, что удобнее оптимизировать его логарифм:\n",
    "\n",
    "$$ - \\sum_{i = 1}^{l} \\big( [y_i = +1] \\log(b(x_i)) + [y_i = -1] \\log(1 - b(x_i)) \\big) \\rightarrow \\min$$\n",
    "\n",
    "Данная функция потерь называется **логарифмической (или кросс-энтропия, log-loss)**.\n",
    "\n",
    "Покажем, что она также позволяет корректно предсказывать вероятности. Запишем матожидание функции потерь в точке x:\n",
    "$$E \\big[ L(y, b) | x \\big] = E \\big[ -[y = +1] \\log(b) - [y = -1] \\log(1 - b) \\big] = $$\n",
    "\n",
    "$$= - p(y = +1 | x) \\log(b) - (1 - p(y = +1 | x)) \\log(1 - b).$$\n",
    "\n",
    "Продифференцируем по b:\n",
    "$$\\frac{\\partial E \\big[ L(y, b) | x \\big] }{\\partial b} = - \\frac{p(y = +1 | x)}{b} + \\frac{1 - p(y = +1 | x)}{1 - b} = 0$$\n",
    "\n",
    "Легко видеть, что оптимальный ответ алгоритма равен вероятности положительного класса:\n",
    "$$b_{*} = p(y = +1 | x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4652aed4",
   "metadata": {},
   "source": [
    "## 3. Логистическая регрессия\n",
    "\n",
    "Везде выше мы требовали, чтобы алгоритм $b(x)$ возвращал числа из отрезка $[0, 1]$. Этого легко достичь, если положить $b(x) = \\sigma(\\langle w, x \\rangle)$, где в качестве $σ$ может выступать любая монотонно неубывающая функция с областью значений $[0, 1]$.\n",
    "\n",
    "Мы будем использовать сигмоидную функцию: $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Таким образом, чем больше скалярное произведение $\\langle w, x \\rangle$, тем больше будет предсказанная вероятность. Как при этом можно интерпретировать данное скалярное произведение? Чтобы ответить на этот вопрос, преобразуем уравнение\n",
    "\n",
    "$$p(y = 1 | x) = \\frac{1}{1 + e^{-\\langle w, x \\rangle}}$$\n",
    "\n",
    "Выражая из него скалярное произведение, получим\n",
    "$$\\langle w, x \\rangle = \\log \\frac{p(y = +1 | x)}{p(y = -1 | x)}$$\n",
    "\n",
    "Получим, что скалярное произведение будет равно логарифму отношения вероятностей классов (log-odds).\n",
    "\n",
    "Как уже упоминалось выше, при использовании квадратичной функции потерь алгоритм будет пытаться предсказывать вероятности, но данная функция потерь является далеко не самой лучшей, поскольку слабо штрафует за грубые ошибки. Логарифмическая функция потерь подходит гораздо лучше, поскольку не позволяет алгоритму сильно ошибаться в вероятностях.\n",
    "\n",
    "Подставим трансформированный ответ линейной модели в логарифмическую функцию потерь:\n",
    "\n",
    "$$- \\sum_{i = 1}^l \\big( [y_i = +1] \\log \\frac{1}{1 + e^{- \\langle w, x_i \\rangle}} + [y_i = -1] \\log \\frac{e^{- \\langle w, x_i \\rangle}}{1 + e^{- \\langle w, x_i \\rangle}} \\big) = $$\n",
    "\n",
    "$$ = - \\sum_{i = 1}^l \\big( [y_i = +1] \\log \\frac{1}{1 + e^{- \\langle w, x_i \\rangle}} + [y_i = -1] \\log \\frac{1}{1 + e^{\\langle w, x_i \\rangle}} \\big) = $$\n",
    "\n",
    "$$ = \\sum_{i = 1}^l \\log (1 + e^{-y_i \\langle w, x_i \\rangle}).$$\n",
    "\n",
    "Линейная модель классификации, настроенная путём минимизации данного функционала, называется **логистической регрессией**. Как видно из приведенных рассуждений, она оптимизирует правдоподобие выборки и дает корректные оценки вероятности принадлежности к положительному классу."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eeccff",
   "metadata": {},
   "source": [
    "## Литература\n",
    "\n",
    "Курс \"Машинное обучение\" на ФКН ВШЭ: https://github.com/esokolov/ml-course-hse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b6352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
